{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/.local/share/virtualenvs/w266_final_project-K7oLnwuw/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/felipe/.local/share/virtualenvs/w266_final_project-K7oLnwuw/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/felipe/.local/share/virtualenvs/w266_final_project-K7oLnwuw/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/felipe/.local/share/virtualenvs/w266_final_project-K7oLnwuw/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/felipe/.local/share/virtualenvs/w266_final_project-K7oLnwuw/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/felipe/.local/share/virtualenvs/w266_final_project-K7oLnwuw/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/felipe/.local/share/virtualenvs/w266_final_project-K7oLnwuw/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/felipe/.local/share/virtualenvs/w266_final_project-K7oLnwuw/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/felipe/.local/share/virtualenvs/w266_final_project-K7oLnwuw/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/felipe/.local/share/virtualenvs/w266_final_project-K7oLnwuw/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/felipe/.local/share/virtualenvs/w266_final_project-K7oLnwuw/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/felipe/.local/share/virtualenvs/w266_final_project-K7oLnwuw/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0806 21:32:13.669926 140166838814528 deprecation_wrapper.py:119] From bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9222949054897513858\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16482725095810747397\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 14407905105758569756\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11330115994\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6359320351922016087\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from time import time\n",
    "import io\n",
    "import re\n",
    "from csv import reader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, TimeDistributed, Activation, Dropout\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "from keras_contrib.metrics import crf_marginal_accuracy\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from modules.bertLayer import BertLayer\n",
    "\n",
    "local_bert_path =   'bert' # change as needed\n",
    "data_path = 'data/train_products.csv'  # path to ner_dataset.csv file , from\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "\n",
    "# make sure that the paths are accessible within the notebook\n",
    "sys.path.insert(0,local_bert_path)\n",
    "sys.path.insert(0,data_path)\n",
    "\n",
    "import optimization\n",
    "import run_classifier\n",
    "import tokenization\n",
    "import run_classifier_with_tfhub\n",
    "\n",
    "# Tensorflow hub path to BERT module of choice\n",
    "bert_url = \"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\"\n",
    "\n",
    "# Define maximal length of input 'sentences' (post tokenization).\n",
    "max_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0806 21:32:18.094089 140166838814528 deprecation_wrapper.py:119] From bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(bert_url)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWord(word, ner):\n",
    "    \"\"\"\n",
    "    Convert a word into a word token and add supplied NER and POS labels. Note that the word can be\n",
    "    tokenized to two or more tokens. Correspondingly, we add - for now - custom 'X' tokens to the labels in order to\n",
    "    maintain the 1:1 mappings between word tokens and labels.\n",
    "\n",
    "    arguments: word, pos label, ner label\n",
    "    \"\"\"\n",
    "\n",
    "    # the dataset contains various '\"\"\"' combinations which we choose to truncate to '\"', etc.\n",
    "    if word == '\"\"\"\"':\n",
    "        word = '\"'\n",
    "    elif word == '``':\n",
    "        word = '`'\n",
    "\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    tokenLength = len(tokens)      # find number of tokens corresponfing to word to later add 'X' tokens to labels\n",
    "\n",
    "    addDict = dict()\n",
    "\n",
    "    addDict['wordToken'] = tokens\n",
    "    #addDict['posToken'] = [pos] + ['posX'] * (tokenLength - 1)\n",
    "    addDict['nerToken'] = [ner] + ['nerX'] * (tokenLength - 1)\n",
    "    addDict['tokenLength'] = tokenLength\n",
    "\n",
    "\n",
    "    return addDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists for sentences, tokens, labels, etc.\n",
    "sentenceList = []\n",
    "sentenceTokenList = []\n",
    "posTokenList = []\n",
    "nerTokenList = []\n",
    "sentLengthList = []\n",
    "\n",
    "# lists for BERT input\n",
    "bertSentenceIDs = []\n",
    "bertMasks = []\n",
    "bertSequenceIDs = []\n",
    "\n",
    "sentence = ''\n",
    "\n",
    "# always start with [CLS] tokens\n",
    "sentenceTokens = ['[CLS]']\n",
    "posTokens = ['[posCLS]']\n",
    "nerTokens = ['[nerCLS]']\n",
    "\n",
    "with open('data/train_products.csv') as csv_file:\n",
    "    csv_reader = reader(csv_file, delimiter=',')\n",
    "    line = 0\n",
    "    for row in csv_reader:\n",
    "        words = row[0].split(\" \")\n",
    "        tags  = row[2].split(\" \")\n",
    "        tags.pop()\n",
    "        #print(tags,words)\n",
    "        \n",
    "        sentenceLength = min(max_length -1, len(sentenceTokens))\n",
    "        sentLengthList.append(sentenceLength)\n",
    "        \n",
    "                    \n",
    "        # Create space for at least a final '[SEP]' token\n",
    "        if sentenceLength >= max_length - 1: \n",
    "            sentenceTokens = sentenceTokens[:max_length - 2]\n",
    "            posTokens = posTokens[:max_length - 2]\n",
    "            nerTokens = nerTokens[:max_length - 2]\n",
    "\n",
    "        # add a ['SEP'] token and padding\n",
    "        \n",
    "        sentenceTokens += ['[SEP]'] + ['[PAD]'] * (max_length -1 - len(sentenceTokens))\n",
    "        #print(sentenceTokens)\n",
    "        posTokens += ['[posSEP]'] + ['[posPAD]'] * (max_length - 1 - len(posTokens) )\n",
    "        nerTokens += ['[nerSEP]'] + ['[nerPAD]'] * (max_length - 1 - len(nerTokens) )\n",
    "            \n",
    "        sentenceList.append(sentence)\n",
    "        \n",
    "        sentenceTokenList.append(sentenceTokens)\n",
    "        \n",
    "       \n",
    "\n",
    "        bertSentenceIDs.append(tokenizer.convert_tokens_to_ids(sentenceTokens))\n",
    "        bertMasks.append([1] * (sentenceLength + 1) + [0] * (max_length -1 - sentenceLength ))\n",
    "        bertSequenceIDs.append([0] * (max_length))\n",
    "                             \n",
    "        posTokenList.append(posTokens)\n",
    "        nerTokenList.append(nerTokens)\n",
    "        \n",
    "        sentence = ''\n",
    "        sentenceTokens = ['[CLS]']\n",
    "        nerTokens = ['[nerCLS]']\n",
    "        \n",
    "        sentence += ' ' + words[0]\n",
    "        i = 0\n",
    "        for word in words:\n",
    "            ner = tags[i]\n",
    "            if i==0:\n",
    "                sentence += ' ' + word\n",
    "            addDict = addWord(word, ner)\n",
    "            #print(addDict)\n",
    "            sentenceTokens += addDict['wordToken']\n",
    "            nerTokens += addDict['nerToken']\n",
    "            i = i + 1\n",
    "        line = line + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentLengthList = sentLengthList\n",
    "sentenceTokenList = sentenceTokenList[2:]\n",
    "bertSentenceIDs = bertSentenceIDs[2:]\n",
    "bertMasks = bertMasks[2:]\n",
    "bertSequenceIDs = bertSequenceIDs[2:]\n",
    "nerTokenList = nerTokenList[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.000e+00, 0.000e+00, 1.000e+00, 2.100e+01, 3.420e+02, 1.300e+03,\n",
       "        2.969e+03, 4.649e+03, 5.861e+03, 6.396e+03, 6.157e+03, 6.633e+03,\n",
       "        5.791e+03, 4.995e+03, 3.955e+03, 3.270e+03, 2.529e+03, 2.028e+03,\n",
       "        1.434e+03, 1.073e+03, 7.560e+02, 5.620e+02, 4.080e+02, 3.210e+02,\n",
       "        1.900e+02, 1.280e+02, 7.100e+01, 3.430e+02]),\n",
       " array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
       "        14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26.,\n",
       "        27., 28., 29.]),\n",
       " <a list of 28 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentenceLengths= [l for l in sentLengthList]\n",
    "\n",
    "plt.hist(np.array(sentenceLengths), bins=(max_length-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bertSentenceIDs)\n",
    "\n",
    "nerClasses = pd.DataFrame(np.array(nerTokenList).reshape(-1))\n",
    "nerClasses.columns = ['tag']\n",
    "nerClasses.tag = pd.Categorical(nerClasses.tag)\n",
    "nerClasses['cat'] = nerClasses.tag.cat.codes\n",
    "nerClasses['sym'] = nerClasses.tag.cat.codes\n",
    "nerLabels = np.array(nerClasses.cat).reshape(numSentences, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f7999dfb898>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nerClasses[['cat']].hist(bins=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-B</td>\n",
       "      <td>0</td>\n",
       "      <td>62244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I-B</td>\n",
       "      <td>1</td>\n",
       "      <td>18692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "      <td>319829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nerCLS]</td>\n",
       "      <td>3</td>\n",
       "      <td>62182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[nerPAD]</td>\n",
       "      <td>4</td>\n",
       "      <td>1021680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[nerSEP]</td>\n",
       "      <td>5</td>\n",
       "      <td>62182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nerX</td>\n",
       "      <td>6</td>\n",
       "      <td>318651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tag  cat  occurences\n",
       "0       B-B    0       62244\n",
       "1       I-B    1       18692\n",
       "2         O    2      319829\n",
       "3  [nerCLS]    3       62182\n",
       "4  [nerPAD]    4     1021680\n",
       "5  [nerSEP]    5       62182\n",
       "6      nerX    6      318651"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerDistribution = (nerClasses.groupby(['tag', 'cat']).agg({'sym':'count'}).reset_index()\n",
    "                   .rename(columns={'sym':'occurences'}))\n",
    "\n",
    "numNerClasses = nerDistribution.tag.nunique()\n",
    "\n",
    "nerDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.798046\n",
       "Name: occurences, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_occurences = nerDistribution.loc[nerDistribution.tag == 'O','occurences']\n",
    "All_occurences = nerDistribution[nerDistribution.cat < 3]['occurences'].sum()\n",
    "\n",
    "O_occurences/All_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = np.array([bertSentenceIDs, bertMasks, bertSequenceIDs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bert_inputs[0])\n",
    "np.random.seed(0)\n",
    "training_examples = np.random.binomial(1, 0.7, numSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentence_ids = []\n",
    "trainMasks = []\n",
    "trainSequence_ids = []\n",
    "\n",
    "testSentence_ids = []\n",
    "testMasks = []\n",
    "testSequence_ids = []\n",
    "\n",
    "nerLabels_train =[]\n",
    "nerLabels_test = []\n",
    "\n",
    "\n",
    "for example in range(numSentences):\n",
    "    if training_examples[example] == 1:\n",
    "        trainSentence_ids.append(bert_inputs[0][example])\n",
    "        trainMasks.append(bert_inputs[1][example])\n",
    "        trainSequence_ids.append(bert_inputs[2][example])\n",
    "        nerLabels_train.append(nerLabels[example])\n",
    "    else:\n",
    "        testSentence_ids.append(bert_inputs[0][example])\n",
    "        testMasks.append(bert_inputs[1][example])\n",
    "        testSequence_ids.append(bert_inputs[2][example])\n",
    "        nerLabels_test.append(nerLabels[example])\n",
    "        \n",
    "X_train = np.array([trainSentence_ids,trainMasks,trainSequence_ids])\n",
    "X_test = np.array([testSentence_ids,testMasks,testSequence_ids])\n",
    "\n",
    "nerLabels_train = np.array(nerLabels_train)\n",
    "nerLabels_test = np.array(nerLabels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a parameter pair k_start, k_end to look at slices. This helps with quick tests.\n",
    "\n",
    "k_start = 0\n",
    "k_end = -1\n",
    "\n",
    "if k_end == -1:\n",
    "    k_end_train = X_train[0].shape[0]\n",
    "    k_end_test = X_test[0].shape[0]\n",
    "else:\n",
    "    k_end_train = k_end_test = k_end\n",
    "    \n",
    "\n",
    "\n",
    "bert_inputs_train_k = [X_train[0][k_start:k_end_train], X_train[1][k_start:k_end_train], \n",
    "                       X_train[2][k_start:k_end_train]]\n",
    "bert_inputs_test_k = [X_test[0][k_start:k_end_test], X_test[1][k_start:k_end_test], \n",
    "                      X_test[2][k_start:k_end_test]]\n",
    "\n",
    "\n",
    "labels_train_k = nerLabels_train[k_start:k_end_train]\n",
    "labels_test_k = nerLabels_test[k_start:k_end_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss function explicitly, filtering out 'extra inserted labels'\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length + 1) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns:  cost\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
    "    \n",
    "    mask = (y_label < 3)   # This mask is used to remove all tokens that do not correspond to the original base text.\n",
    "\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)  # mask the labels\n",
    "    \n",
    "    y_flat_pred = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float32)),[-1, numNerClasses])\n",
    "    \n",
    "    y_flat_pred_masked = tf.boolean_mask(y_flat_pred, mask) # mask the predictions\n",
    "    \n",
    "    return tf.reduce_mean(sparse_categorical_crossentropy(y_label_masked, y_flat_pred_masked,from_logits=False ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_acc_orig_tokens(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction filtering out also the newly inserted labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_label < 3)\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, numNerClasses]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_acc_orig_non_other_tokens(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction explicitly filtering out also the 'Other'- labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_label < 2)\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, numNerClasses]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_customized = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.91, beta_2=0.999, epsilon=None, decay=0.1, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Create BERT layer, following https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
    "    init:  initialize layer. Specify various parameters regarding output types and dimensions. Very important is\n",
    "           to set the number of trainable layers.\n",
    "    build: build the layer based on parameters\n",
    "    call:  call the BERT layer within a model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"sequence\",\n",
    "        bert_url=\"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_url = bert_url\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_url, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "        trainable_layers = []\n",
    "\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "        mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_model(max_input_length, train_layers, optimizer):\n",
    "    \"\"\"\n",
    "    Implementation of NER model\n",
    "    \n",
    "    variables:\n",
    "        max_input_length: number of tokens (max_length + 1)\n",
    "        train_layers: number of layers to be retrained\n",
    "        optimizer: optimizer to be used\n",
    "    \n",
    "    returns: model\n",
    "    \"\"\"\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_input_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_input_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_input_length,), name=\"segment_ids\")\n",
    "    \n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    bert_sequence = BertLayer(n_fine_tune_layers=train_layers)(bert_inputs)\n",
    "    \n",
    "    #print(bert_sequence)\n",
    "    \n",
    "    lstm = Bidirectional(LSTM(256, return_sequences=True))(bert_sequence)\n",
    "    \n",
    "    dense = TimeDistributed(Dense(256, activation='relu'))(lstm)\n",
    "    \n",
    "    dense = Dropout(rate=0.1)(dense)\n",
    "    \n",
    "    dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(7, activation='softmax', name='ner')(dense)\n",
    "     \n",
    "    print('pred: ', pred)\n",
    "    \n",
    "    ## Prepare for multipe loss functions, although not used here\n",
    "    \n",
    "    losses = {\n",
    "        \"ner\": custom_loss,\n",
    "        }\n",
    "    lossWeights = {\"ner\": 1.0\n",
    "                  }\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "\n",
    "    model.compile(loss=losses, optimizer=optimizer, metrics=[custom_acc_orig_tokens, \n",
    "                                                          custom_acc_orig_non_other_tokens])\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTrainSentences = 1000\n",
    "validation_split = 0.5\n",
    "np.random.seed(seed=0)\n",
    "indices = np.arange(bert_inputs_train_k[0].shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "bert_inputs_train_reordered = [bert_inputs_train_k[0][indices], \\\n",
    "                          bert_inputs_train_k[1][indices], \\\n",
    "                          bert_inputs_train_k[2][indices]]\n",
    "\n",
    "labels_train_reordered = labels_train_k[indices]\n",
    "\n",
    "nb_validation_samples = int(validation_split * numTrainSentences)\n",
    "\n",
    "end_validation_samples = int(nb_validation_samples+numTrainSentences*(1-validation_split))\n",
    "\n",
    "bert_inputs_train_tiny = [bert_inputs_train_reordered[0][:nb_validation_samples], \\\n",
    "                          bert_inputs_train_reordered[1][:nb_validation_samples], \\\n",
    "                          bert_inputs_train_reordered[2][:nb_validation_samples]]\n",
    "\n",
    "bert_inputs_validate_tiny = [bert_inputs_train_reordered[0][nb_validation_samples:end_validation_samples], \\\n",
    "                          bert_inputs_train_reordered[1][nb_validation_samples:end_validation_samples], \\\n",
    "                          bert_inputs_train_reordered[2][nb_validation_samples:end_validation_samples]]\n",
    "\n",
    "\n",
    "y_train = labels_train_reordered[:nb_validation_samples]\n",
    "#x_val = data[-nb_validation_samples:]\n",
    "y_val = labels_train_reordered[nb_validation_samples:end_validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0806 21:32:42.835906 140166838814528 deprecation.py:506] From /home/felipe/.local/share/virtualenvs/w266_final_project-K7oLnwuw/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0806 21:32:42.842859 140166838814528 deprecation.py:506] From /home/felipe/.local/share/virtualenvs/w266_final_project-K7oLnwuw/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0806 21:32:42.844242 140166838814528 deprecation.py:506] From /home/felipe/.local/share/virtualenvs/w266_final_project-K7oLnwuw/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0806 21:32:42.845175 140166838814528 deprecation.py:506] From /home/felipe/.local/share/virtualenvs/w266_final_project-K7oLnwuw/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:  Tensor(\"ner/truediv:0\", shape=(?, ?, 7), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0806 21:32:43.679526 140166838814528 deprecation.py:323] From /home/felipe/.local/share/virtualenvs/w266_final_project-K7oLnwuw/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, None, 768)    178565115   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, None, 512)    2099200     bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 256)    131328      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 256)    0           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 256)    0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ner (Dense)                     (None, None, 7)      1799        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 180,797,442\n",
      "Trainable params: 2,232,327\n",
      "Non-trainable params: 178,565,115\n",
      "__________________________________________________________________________________________________\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/8\n",
      "500/500 [==============================] - 10s 20ms/sample - loss: 0.7063 - custom_acc_orig_tokens: 0.7523 - custom_acc_orig_non_other_tokens: 0.0275 - val_loss: 0.4995 - val_custom_acc_orig_tokens: 0.8052 - val_custom_acc_orig_non_other_tokens: 0.0363\n",
      "Epoch 2/8\n",
      "500/500 [==============================] - 7s 13ms/sample - loss: 0.4248 - custom_acc_orig_tokens: 0.8330 - custom_acc_orig_non_other_tokens: 0.2743 - val_loss: 0.4414 - val_custom_acc_orig_tokens: 0.8291 - val_custom_acc_orig_non_other_tokens: 0.1875\n",
      "Epoch 3/8\n",
      "500/500 [==============================] - 7s 13ms/sample - loss: 0.3345 - custom_acc_orig_tokens: 0.8697 - custom_acc_orig_non_other_tokens: 0.4904 - val_loss: 0.3601 - val_custom_acc_orig_tokens: 0.8522 - val_custom_acc_orig_non_other_tokens: 0.5117\n",
      "Epoch 4/8\n",
      "500/500 [==============================] - 7s 13ms/sample - loss: 0.2374 - custom_acc_orig_tokens: 0.9139 - custom_acc_orig_non_other_tokens: 0.6910 - val_loss: 0.3146 - val_custom_acc_orig_tokens: 0.8774 - val_custom_acc_orig_non_other_tokens: 0.6181\n",
      "Epoch 5/8\n",
      "500/500 [==============================] - 7s 13ms/sample - loss: 0.1704 - custom_acc_orig_tokens: 0.9397 - custom_acc_orig_non_other_tokens: 0.7813 - val_loss: 0.3051 - val_custom_acc_orig_tokens: 0.8932 - val_custom_acc_orig_non_other_tokens: 0.6859\n",
      "Epoch 6/8\n",
      "500/500 [==============================] - 7s 13ms/sample - loss: 0.0984 - custom_acc_orig_tokens: 0.9688 - custom_acc_orig_non_other_tokens: 0.8979 - val_loss: 0.3110 - val_custom_acc_orig_tokens: 0.8942 - val_custom_acc_orig_non_other_tokens: 0.7181\n",
      "Epoch 7/8\n",
      "500/500 [==============================] - 7s 13ms/sample - loss: 0.0569 - custom_acc_orig_tokens: 0.9831 - custom_acc_orig_non_other_tokens: 0.9413 - val_loss: 0.3328 - val_custom_acc_orig_tokens: 0.9021 - val_custom_acc_orig_non_other_tokens: 0.6769\n",
      "Epoch 8/8\n",
      "500/500 [==============================] - 7s 13ms/sample - loss: 0.0332 - custom_acc_orig_tokens: 0.9891 - custom_acc_orig_non_other_tokens: 0.9684 - val_loss: 0.3854 - val_custom_acc_orig_tokens: 0.9026 - val_custom_acc_orig_non_other_tokens: 0.6833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f798e374160>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "model = ner_model(max_length,train_layers=0,optimizer='adam')\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "model.fit(\n",
    "    bert_inputs_train_tiny, \n",
    "    {\"ner\": y_train },\n",
    "    validation_data=(bert_inputs_validate_tiny, {\"ner\": y_val}),\n",
    "    epochs=8,\n",
    "    batch_size=32#,\n",
    "    #callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs_infer = [X_test[0], X_test[1], X_test[2]]\n",
    "\n",
    "result = model.predict(\n",
    "    bert_inputs_infer, \n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_flat = [pred for preds in np.argmax(result, axis=2) for pred in preds]\n",
    "labels_flat = [label for labels in nerLabels_test for label in labels]\n",
    "\n",
    "clean_preds = []\n",
    "clean_labels = []\n",
    "\n",
    "for pred, label in zip(predictions_flat, labels_flat):\n",
    "    if label < 3:\n",
    "        clean_preds.append(pred)\n",
    "        clean_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = list(nerDistribution['tag'])[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         B-B      0.766     0.721     0.743     18579\n",
      "         I-B      0.918     0.483     0.633      5637\n",
      "           O      0.929     0.965     0.947     95368\n",
      "\n",
      "    accuracy                          0.905    119584\n",
      "   macro avg      0.871     0.723     0.774    119584\n",
      "weighted avg      0.903     0.905     0.900    119584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(clean_labels, clean_preds, target_names=target_names, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266-env",
   "language": "python",
   "name": "w266-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
