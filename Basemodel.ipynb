{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TEXTS = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, tags, brands = [], [], []\n",
    "with open(\"data/train_products.csv\", 'r') as f:\n",
    "    reader = csv.DictReader(f, fieldnames=[\"title\",\"brand\",\"tags\"])\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        #print(row)\n",
    "        count += 1\n",
    "        text, tag_set = row['title'], row['tags'].split(' ')[:-1]\n",
    "        texts.append(text)\n",
    "        tags.append(tag_set)\n",
    "        brands.append(row['brand'])\n",
    "        if count >= MAX_TEXTS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts  = np.asarray(texts)\n",
    "brands = np.asarray(brands)\n",
    "tags   = np.asarray(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62184"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)\n",
    "indices = np.arange(texts.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "texts_reordered  = texts[indices].tolist()\n",
    "brands_reordered = brands[indices].tolist()\n",
    "tags_reordered   = tags[indices].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(texts_reordered)\n",
    "np.random.seed(0)\n",
    "training_examples = np.random.binomial(1, 0.7, numSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentence = []\n",
    "testSentence = []\n",
    "\n",
    "\n",
    "nerLabels_train =[]\n",
    "nerLabels_test = []\n",
    "\n",
    "\n",
    "for example in range(numSentences):\n",
    "    if training_examples[example] == 1:\n",
    "        trainSentence.append(texts_reordered[example])\n",
    "        nerLabels_train.append(brands_reordered[example])\n",
    "    else:\n",
    "        testSentence.append(texts_reordered[example])\n",
    "        nerLabels_test.append(brands_reordered[example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a parameter pair k_start, k_end to look at slices. This helps with quick tests.\n",
    "\n",
    "k_start = 0\n",
    "k_end = 500\n",
    "#k_end = -1\n",
    "\n",
    "if k_end == -1:\n",
    "    k_end_train = len(trainSentence)\n",
    "    k_end_test = len(testSentence)\n",
    "else:\n",
    "    k_end_train = k_end\n",
    "    \n",
    "\n",
    "\n",
    "trainSentence_k  = trainSentence[k_start:k_end_train]\n",
    "nerLabels_train_k   = list(set(nerLabels_train[k_start:k_end_train]))[0:len(list(set(nerLabels_train[k_start:k_end_train])))-79]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(nerLabels_train_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nerLabels_predict = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for sentence in testSentence:\n",
    "    sentDict = {}\n",
    "    sentDict[\"sentence\"]       = sentence\n",
    "    sentDict[\"label\"]          = nerLabels_test[i]\n",
    "    for label in set(nerLabels_train_k):\n",
    "        if label in sentence:\n",
    "            sentDict[\"pre_label\"] = label\n",
    "            break\n",
    "    nerLabels_predict.append(sentDict)\n",
    "    i = i + 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictionDF = pd.DataFrame(nerLabels_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionDF[\"pre_label\"] = predictionDF[\"pre_label\"].fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_brands(brand,title):\n",
    "    tagging = ''\n",
    "    brand = brand.split(' ')\n",
    "    brand_started = False\n",
    "    not_pass = False\n",
    "    i = 0\n",
    "    added_i = 0\n",
    "    words = title.split(' ')\n",
    "    for word in title.split(' '):\n",
    "        if word == brand[0] and not_pass is False:\n",
    "            tagging += 'B-B '\n",
    "            brand_started = True\n",
    "        elif len(brand) > 1 and brand_started:\n",
    "            j = i\n",
    "            for b in brand[1:]:\n",
    "                #print(b,words[j],words,brand)\n",
    "                if words[j] == b:\n",
    "                    tagging += 'I-B '\n",
    "                    added_i = added_i + 1\n",
    "                else:\n",
    "                    brand_started = False\n",
    "                    tagging += 'O '\n",
    "                    added_i = added_i + 1\n",
    "                    \n",
    "                j = j + 1\n",
    "            brand_started = False\n",
    "            not_pass = True\n",
    "        else:\n",
    "            brand_started = False\n",
    "            if added_i >= 2:\n",
    "                added_i = added_i - 1\n",
    "            else:\n",
    "                tagging += 'O '\n",
    "                \n",
    "        i = i + 1\n",
    "    #return tagging\n",
    "    #print(\"Words\",tagging)\n",
    "    tags = tagging.split(\" \")\n",
    "    tags.pop()\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionDF['tags']           = predictionDF.apply(lambda x: tag_brands(x['label'],x['sentence']), axis=1)\n",
    "predictionDF['predicted_tags'] = predictionDF.apply(lambda x: tag_brands(x['pre_label'],x['sentence']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionDF['tags_length'] = predictionDF['tags'].str.len()\n",
    "predictionDF['predicted_tags_length'] = predictionDF['predicted_tags'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = list(itertools.chain.from_iterable(list(predictionDF['tags'])))\n",
    "y_pred = list(itertools.chain.from_iterable(list(predictionDF['predicted_tags'])))\n",
    "target_names = ['0', 'B-B', 'I-B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94     95691\n",
      "         B-B       0.96      0.48      0.64     18592\n",
      "         I-B       1.00      0.55      0.71      5531\n",
      "\n",
      "    accuracy                           0.90    119814\n",
      "   macro avg       0.95      0.67      0.76    119814\n",
      "weighted avg       0.90      0.90      0.88    119814\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, labels = ['O','B-B','I-B'], target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionDF[predictionDF['tags_length']!=predictionDF['predicted_tags_length']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266-env",
   "language": "python",
   "name": "w266-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
